{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMT Error Matching <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the `measure_errors` script to match the proportion of errors present\n",
    "in a given set of transcriptions.\n",
    "\n",
    "The script outputs a `config.json` file which can either be passed to the\n",
    "`make_dataset` script to create a static ACME dataset of the given proportions,\n",
    "or it can be used to instantiate a `Degrader`, which can create degraded data\n",
    "on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ACME\n",
    "\n",
    "We'll first create a small ACME dataset using only the PianoMidi data. For this example, we'll use the degraded data as our \"transcribed\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python ../make_dataset.py --datasets PianoMidi --no-prompt --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Metadata\n",
    "\n",
    "We'll need to load the dataset's metadata in order to use it as our pseudo-transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "degradation_ids = pd.read_csv(os.path.join('acme', 'degradation_ids.csv'))\n",
    "\n",
    "metadata = pd.read_csv(os.path.join('acme', 'metadata.csv'))\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize the \"AMT\" Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "trans_dir = os.path.join(\"error_matching\", \"trans\")\n",
    "gt_dir = os.path.join(\"error_matching\", \"gt\")\n",
    "\n",
    "os.makedirs(trans_dir, exist_ok=True)\n",
    "os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    shutil.copy(os.path.join('acme', row.clean_csv_path), gt_dir)\n",
    "    shutil.copy(os.path.join('acme', row.altered_csv_path), trans_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, error_matching contains two directories:\n",
    "* `gt`: Contains the ground truth clean excerpts.\n",
    "* `trans`: Contains the \"transcribed\" excerpts.\n",
    "\n",
    "Matching ground truth and transcribed files _must_ have the same basename\n",
    "in their respective directories. The files can be CSV, MIDI, or pickle (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Measure Errors script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python ../measure_errors.py --trans error_matching/trans --gt error_matching/gt --excerpt-length 15000 --min-notes 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `measure_errors` script can be run on full piece transcriptions, so you can set an `excerpt-length` and `min-notes` via command line arguments. In our case, since we are running in only on excerpts, we set the `excerpt-length` to be longer than all of the excerpts, and the minimum number of notes to be smaller than all excerpts.\n",
    "\n",
    "For the full usage, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from mdtk.degradations import DEGRADATIONS\n",
    "\n",
    "with open(\"config.json\", \"r\") as json_file:\n",
    "    config = json.load(json_file)\n",
    "\n",
    "print(\"Degradation probabilities:\")\n",
    "for prob, deg in zip(config['degradation_dist'], DEGRADATIONS.keys()):\n",
    "    print(f\"{deg.rjust(14)}: {prob}\")\n",
    "\n",
    "print()\n",
    "print(f\"Clean_prop: {config['clean_prop']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the probabilities are not all `1/9`, as might be expected from the ACME creation.\n",
    "This is because the measured probabilities only find one possible path from each transcription\n",
    "to its ground truth. In our case, it seems that many of the `time_shifts` are classified as\n",
    "a `remove_note` and an `add_note`. For the `measure_errors` script to classify an error as a `time_shift`,\n",
    "the shift length must be smaller than the duration of the shifted note. Without giving any arguments\n",
    "to `make_dataset`, this is unlikely.\n",
    "\n",
    "Let's take a few examples specifically. First, let's pick a random pitch shift, and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdtk.fileio import csv_to_df\n",
    "\n",
    "from utils import plot_from_df\n",
    "\n",
    "def get_random_dfs(deg_name, metadata):\n",
    "    deg_id = degradation_ids.loc[degradation_ids[\"degradation_name\"] == deg_name, \"id\"].values[0]\n",
    "    meta_df = metadata.loc[metadata[\"degradation_id\"] == deg_id]\n",
    "\n",
    "    row = meta_df.sample()\n",
    "    basename = os.path.basename(row[\"clean_csv_path\"].values[0])\n",
    "    gt_df = csv_to_df(os.path.join(gt_dir, basename))\n",
    "    trans_df = csv_to_df(os.path.join(trans_dir, basename))\n",
    "\n",
    "    return gt_df, trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt_df, trans_df = get_random_dfs(\"pitch_shift\", metadata)\n",
    "\n",
    "plot_from_df(gt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_df(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's measure the degradation present in the selected excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from measure_errors import get_excerpt_degs\n",
    "\n",
    "deg_counts = get_excerpt_degs(gt_df, trans_df)\n",
    "\n",
    "print(\"Degradation counts:\")\n",
    "for count, deg in zip(deg_counts, DEGRADATIONS.keys()):\n",
    "    print(f\"{deg.rjust(14)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try the same for time_shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df, trans_df = get_random_dfs(\"time_shift\", metadata)\n",
    "\n",
    "plot_from_df(gt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_df(trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deg_counts = get_excerpt_degs(gt_df, trans_df)\n",
    "\n",
    "print(\"Degradation counts:\")\n",
    "for count, deg in zip(deg_counts, DEGRADATIONS.keys()):\n",
    "    print(f\"{deg.rjust(14)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Custom Dataset\n",
    "\n",
    "You can feed the generated `config.json` file to `make_dataset.py` in order to generate a custom ACME dataset matching the measured degradation and clean proportions:\n",
    "\n",
    "```bash\n",
    "  python ../make_dataset.py --datasets PianoMidi --no-prompt --seed 42 --config config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input file types\n",
    "\n",
    "The measure_errors script can read `MIDI`, `CSV`, or `pickle` files:\n",
    "* `MIDI`: Any MIDI file.\n",
    "* `CSV`: Any CSV file generated by mdtk (see mdtk/fileio.py).\n",
    "* `pickle`: A pickle file containing a single numpy array called `piano_roll`, of shape either `num_frames x num_pitches`, or `num_frames x (2 * num_pitches)`, in which case the first `num_pitches` columns are a note presence piano roll and the last `num_pitches` columns are a corresponding onset piano roll. The min and max pitch can be set using `--pr-min-pitch` and `--pr-max-pitch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command line arguments\n",
    "The measure_errors script has many other command line arguments. A full list is below, but we will highlight a few of the most useful ones here:\n",
    "\n",
    "* `--trans` and `--gt`: Directories in which the script will look for matching ground truths and transcriptions. Any files which match in basename (not including extension) will be treated as matches. For example, the ground truth 'file1.mid' will match the transcription 'file1.csv'. Include `-r` to search directories recursively.\n",
    "* `--trans_start` and `--trans_end`: If the transcriptions are only partial transcriptions of the ground truths, these arguments can be used to set the bounds of the transcriptions (in ms). For example, if only the first 30 seconds are transcribed, use `--trans_end 30000`.\n",
    "* `--excerpt-length`: Each transcription is split into excerpts of this length before errors are measured. This should be set to the length of excerpt which you plan to send to your model. Shorter values will lead to more accurate error measurements, but longer values may contain long-range patterns that aid in modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../measure_errors.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "! python ../make_dataset.py --no-prompt --clean\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "shutil.rmtree(\"acme\")\n",
    "shutil.rmtree(\"error_matching\")\n",
    "os.remove(\"config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
